{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\data\\myenv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\data\\myenv\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\data\\myenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\data\\myenv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\data\\myenv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\data\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (2051, 6)\n",
      "After removing duplicates: (2001, 6)\n",
      "Removing 36 invalid email entries\n",
      "Removing 23 invalid age entries\n",
      "Final cleaned data shape: (1942, 6)\n",
      "\n",
      "Sample cleaned data:\n",
      "   user_id             name                         email  age  \\\n",
      "0    15000      James Scott      james.scott660@gmail.com   35   \n",
      "1    15001   Sharon Ferrell    sharon.ferrell49@gmail.com   26   \n",
      "2    15002    Chelsea Ortiz    chelsea.ortiz643@gmail.com   38   \n",
      "3    15003  Gregory Pearson  gregory.pearson379@gmail.com   52   \n",
      "4    15004  Connie Gonzales  connie.gonzales481@gmail.com   43   \n",
      "\n",
      "          country       date  \n",
      "0           India 2024-11-01  \n",
      "1       Australia 2024-11-01  \n",
      "2  United Kingdom 2024-11-01  \n",
      "3  United Kingdom 2024-11-01  \n",
      "4       Australia 2024-11-01  \n",
      "\n",
      "Validation Report:\n",
      "Original entries: 165\n",
      "Final valid entries: 1942\n",
      "Invalid entries removed: -1777\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_users(input_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    print(\"Original data shape:\", df.shape)\n",
    "    \n",
    "    df = df.drop_duplicates(subset='user_id', keep='last')\n",
    "    print(\"After removing duplicates:\", df.shape)\n",
    "\n",
    "    email_regex = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    \n",
    "    df['email'] = df['email'].str.replace('@@', '@', regex=False)\n",
    "    df['email'] = df['email'].str.replace('..com', '.com', regex=False)\n",
    "    df['email'] = df['email'].str.replace(' ', '')\n",
    "\n",
    "    valid_emails = df['email'].apply(lambda x: bool(re.match(email_regex, str(x))))\n",
    "    invalid_email_count = len(df[~valid_emails])\n",
    "    print(f\"Removing {invalid_email_count} invalid email entries\")\n",
    "    df = df[valid_emails].copy()\n",
    "\n",
    "    age_mask = df['age'].between(13, 100)\n",
    "    invalid_age_count = len(df[~age_mask])\n",
    "    print(f\"Removing {invalid_age_count} invalid age entries\")\n",
    "    df = df[age_mask].copy()\n",
    "\n",
    "    country_mapping = {\n",
    "        'US': 'United States',\n",
    "        'USA': 'United States',\n",
    "        'UK': 'United Kingdom',\n",
    "        'NULL': 'Unknown',\n",
    "        'XYZ': 'Unknown'\n",
    "    }\n",
    "    \n",
    "    df['country'] = (\n",
    "        df['country']\n",
    "        .str.strip()\n",
    "        .replace(country_mapping)\n",
    "        .fillna('Unknown')\n",
    "        .apply(lambda x: x if x in df['country'].unique() else 'Unknown')\n",
    "    )\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df[df['date'].notna()].copy()\n",
    "\n",
    "    df['name'] = (\n",
    "        df['name']\n",
    "        .str.replace(r'\\s+', ' ', regex=True)\n",
    "        .str.replace(r'[^\\w\\s.]', '', regex=True)\n",
    "        .str.title()\n",
    "    )\n",
    "\n",
    "    df = df.dropna(subset=['email', 'country']).reset_index(drop=True)\n",
    "    \n",
    "    print(\"Final cleaned data shape:\", df.shape)\n",
    "    print(\"\\nSample cleaned data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "cleaned_users = clean_users(\"users.csv\")\n",
    "cleaned_users.to_csv(\"cleaned_users.csv\", index=False)\n",
    "\n",
    "validation_report = {\n",
    "    'original_entries': 165,\n",
    "    'final_entries': len(cleaned_users),\n",
    "    'duplicates_removed': 165 - len(cleaned_users)\n",
    "}\n",
    "\n",
    "print(\"\\nValidation Report:\")\n",
    "print(f\"Original entries: {validation_report['original_entries']}\")\n",
    "print(f\"Final valid entries: {validation_report['final_entries']}\")\n",
    "print(f\"Invalid entries removed: {validation_report['original_entries'] - validation_report['final_entries']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      plan_type  total_users  churned_users  churn_rate\n",
      "0       Premium            8              1       12.50\n",
      "1      Standard          197             42       21.32\n",
      "2  Student Plan          195             49       25.13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "subscriptions = pd.read_csv(\"subscriptions.csv\")\n",
    "\n",
    "# Remove duplicates\n",
    "subscriptions = subscriptions.drop_duplicates(subset='payment_id')\n",
    "\n",
    "# Fix incorrect amounts\n",
    "def correct_amount(row):\n",
    "    plan = row['plan_type']\n",
    "    if plan == 'Premium' and row['amount'] != 12.99:\n",
    "        return 12.99\n",
    "    elif plan == 'Standard' and row['amount'] != 7.99:\n",
    "        return 7.99\n",
    "    elif plan == 'Student Plan' and row['amount'] not in [4.99, -5.99]:\n",
    "        return 4.99\n",
    "    return abs(row['amount'])\n",
    "\n",
    "subscriptions['amount'] = subscriptions.apply(correct_amount, axis=1)\n",
    "\n",
    "# Validate plan types\n",
    "valid_plans = ['Premium', 'Standard', 'Student Plan']\n",
    "subscriptions = subscriptions[subscriptions['plan_type'].isin(valid_plans)]\n",
    "\n",
    "# Ensure churn_status aligns with renewal status\n",
    "subscriptions['churn_status'] = subscriptions['renewed'].apply(lambda x: 'churned' if x == False else 'active')\n",
    "\n",
    "# Validate date relationships\n",
    "subscriptions['payment_date'] = pd.to_datetime(subscriptions['payment_date'])\n",
    "subscriptions['valid_until'] = pd.to_datetime(subscriptions['valid_until'])\n",
    "\n",
    "def validate_dates(row):\n",
    "    if row['plan_type'] == 'Premium':\n",
    "        expected_end = row['payment_date'] + pd.DateOffset(months=2)\n",
    "    else:\n",
    "        expected_end = row['payment_date'] + pd.DateOffset(months=1)\n",
    "    return row['valid_until'] == expected_end\n",
    "\n",
    "subscriptions = subscriptions[subscriptions.apply(validate_dates, axis=1)]\n",
    "\n",
    "# Churn rate analysis\n",
    "churn_analysis = subscriptions.groupby('plan_type').agg(\n",
    "    total_users=('user_id', 'count'),\n",
    "    churned_users=('churn_status', lambda x: (x == 'churned').sum())\n",
    ").reset_index()\n",
    "\n",
    "churn_analysis['churn_rate'] = round(\n",
    "    (churn_analysis['churned_users'] / churn_analysis['total_users']) * 100, 2\n",
    ")\n",
    "\n",
    "subscriptions.to_csv(\"cleaned_subscriptions.csv\", index=False)\n",
    "churn_analysis.to_csv(\"subscription_analysis.csv\", index=False)\n",
    "\n",
    "print(churn_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and analysis complete!\n",
      "\n",
      "Top 5 Movies by Rating:\n",
      "      movie_id  total_ratings  average_rating\n",
      "29          33              1             5.0\n",
      "22          26              1             5.0\n",
      "3118      4169              1             5.0\n",
      "2085      2815              2             5.0\n",
      "1534      2074              1             5.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "\n",
    "# --- Data Cleaning ---\n",
    "# 1. Remove duplicates\n",
    "ratings = ratings.drop_duplicates(subset='rating_id')\n",
    "ratings = ratings.drop_duplicates(subset=['user_id', 'movie_id'])\n",
    "\n",
    "# 2. Fix invalid ratings\n",
    "ratings['rating'] = ratings['rating'].clip(lower=0, upper=5)\n",
    "ratings['rating'] = (ratings['rating'] * 2).round() / 2\n",
    "\n",
    "# 3. Handle missing values\n",
    "median_rating = ratings['rating'].median()\n",
    "ratings['rating'] = ratings['rating'].fillna(median_rating)\n",
    "most_common_date = ratings['review_date'].mode()[0]\n",
    "ratings['review_date'] = ratings['review_date'].fillna(most_common_date)\n",
    "\n",
    "# 4. Validate review dates\n",
    "ratings['review_date'] = pd.to_datetime(ratings['review_date'], errors='coerce')\n",
    "ratings = ratings[ratings['review_date'] >= pd.to_datetime('2024-11-01')]\n",
    "\n",
    "# --- Analysis ---\n",
    "movie_ratings = ratings.groupby('movie_id').agg(\n",
    "    total_ratings=('rating', 'count'),\n",
    "    average_rating=('rating', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "user_stats = ratings.groupby('user_id').agg(\n",
    "    ratings_given=('rating', 'count'),\n",
    "    avg_rating=('rating', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Save cleaned data & analysis\n",
    "ratings.to_csv(\"cleaned_ratings.csv\", index=False)\n",
    "movie_ratings.to_csv(\"movie_ratings_analysis.csv\", index=False)\n",
    "user_stats.to_csv(\"user_behavior_analysis.csv\", index=False)\n",
    "\n",
    "print(\"Cleaning and analysis complete!\")\n",
    "print(\"\\nTop 5 Movies by Rating:\")\n",
    "print(movie_ratings.sort_values(by='average_rating', ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (98923, 6)\n",
      "Removed 0 duplicate watch_id entries\n",
      "Removed 2706 entries with invalid user_ids\n",
      "Removed 725 invalid watch durations\n",
      "Standardized 771 device entries to 'Unknown'\n",
      "Removed 0 invalid dates\n",
      "Final cleaned data shape: (95492, 6)\n",
      "Cleaned data saved to cleaned_watch_history.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_watch_history(watch_file, cleaned_users_file, output_file):\n",
    "    watch_df = pd.read_csv(watch_file)\n",
    "    users_df = pd.read_csv(cleaned_users_file)\n",
    "    \n",
    "    print(\"Original data shape:\", watch_df.shape)\n",
    "    \n",
    "    initial_count = len(watch_df)\n",
    "    watch_df = watch_df.drop_duplicates(subset='watch_id', keep='first')\n",
    "    duplicates_removed = initial_count - len(watch_df)\n",
    "    print(f\"Removed {duplicates_removed} duplicate watch_id entries\")\n",
    "    \n",
    "    valid_user_ids = users_df['user_id'].unique()\n",
    "    valid_user_mask = watch_df['user_id'].isin(valid_user_ids)\n",
    "    invalid_user_count = len(watch_df[~valid_user_mask])\n",
    "    watch_df = watch_df[valid_user_mask].copy()\n",
    "    print(f\"Removed {invalid_user_count} entries with invalid user_ids\")\n",
    "    \n",
    "    duration_mask = watch_df['watch_duration'].between(1, 240)\n",
    "    invalid_duration = len(watch_df[~duration_mask])\n",
    "    watch_df = watch_df[duration_mask].copy()\n",
    "    print(f\"Removed {invalid_duration} invalid watch durations\")\n",
    "    \n",
    "    valid_devices = ['Laptop', 'Mobile', 'Smart TV']\n",
    "    watch_df['device_type'] = (\n",
    "        watch_df['device_type']\n",
    "        .replace({'': 'Unknown'})\n",
    "        .where(watch_df['device_type'].isin(valid_devices), 'Unknown')\n",
    "    )\n",
    "    invalid_device_count = len(watch_df[watch_df['device_type'] == 'Unknown'])\n",
    "    print(f\"Standardized {invalid_device_count} device entries to 'Unknown'\")\n",
    "\n",
    "    watch_df['watch_date'] = pd.to_datetime(watch_df['watch_date'], errors='coerce')\n",
    "    date_mask = watch_df['watch_date'].notna()\n",
    "    invalid_dates = len(watch_df[~date_mask])\n",
    "    watch_df = watch_df[date_mask].copy()\n",
    "    print(f\"Removed {invalid_dates} invalid dates\")\n",
    "\n",
    "    watch_df = watch_df.dropna(subset=['watch_duration'])\n",
    "    print(f\"Final cleaned data shape: {watch_df.shape}\")\n",
    "    \n",
    "    watch_df.to_csv(output_file, index=False)\n",
    "    print(f\"Cleaned data saved to {output_file}\")\n",
    "\n",
    "clean_watch_history(\n",
    "    watch_file=\"watch_history.csv\",\n",
    "    cleaned_users_file=\"cleaned_users.csv\",\n",
    "    output_file=\"cleaned_watch_history.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to cleaned_movies.csv. Removed 4221 entries.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_movies(input_path, output_path):\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    df[\"release_year\"] = df[\"release_year\"].astype(\"Int64\")\n",
    "    df = df[df[\"runtime\"].between(60, 240)]\n",
    "    df[\"genre\"] = df[\"genre\"].str.title()\n",
    "    df[\"title\"] = df[\"title\"].str.strip()\n",
    "\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Cleaned data saved to {output_path}. Removed {len(df)} entries.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_movies(\"movies.csv\", \"cleaned_movies.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
